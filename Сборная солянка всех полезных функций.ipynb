{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт библиотек, всё в одном, что может быть вам полезно. Если чего-то не хватает - устанавливаем, разбираемся. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вариант загрузки\n",
    "from __future__ import division, print_function\n",
    "# отключим всякие предупреждения Anaconda\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# настройка графиков\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15, 8)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "# Перм. импотанс\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "# Основа\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "# Оптимизация\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import minimize_scalar\n",
    "# Склерн\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import cross_validation,model_selection, preprocessing, metrics,preprocessing\n",
    "from sklearn.base import BaseEstimator, TransformerMixin,RegressorMixin, clone\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor, RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score,mean_squared_error,roc_auc_score, silhouette_samples, silhouette_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split,GridSearchCV\n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelEncoder, MinMaxScaler,  Imputer, LabelBinarizer, OneHotEncoder,RobustScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "#Керас\n",
    "from keras.datasets import mnist,cifar10\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Flatten, Activation,Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from collections import defaultdict\n",
    "#Имена парсер\n",
    "import nltk\n",
    "from nameparser.parser import HumanName\n",
    "from nltk.corpus import wordnet\n",
    "#Бустинги\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from w2v_utils import *\n",
    "\n",
    "#Временные ряды\n",
    "from autoarima import AutoArima \n",
    "import takeda_constants as tc\n",
    "#Коннект к БД\n",
    "from sqlalchemy import create_engine, table, column, func\n",
    "pg_engine_url = ''\n",
    "engine = create_engine(pg_engine_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предпроцессинг и базовые функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Парсер каких-то дат, может быть полезен, как из разныех тектовых полей собрать даты\n",
    "def pars(date_frame): \n",
    "    date_frame['Year FY'] = date_frame['Year FY'].apply(lambda row: row.replace(\"FY\" , \"\"))\n",
    "    date_frame['Month FY'] = date_frame['Month FY'].apply(lambda row: row.replace(\"FM\" , \"\").replace(\"_\",\"\").replace('10Jan','01').replace('11Feb','02').replace('12Mar','03').replace('01Apr','04').replace('02May','05').replace('03Jun','06').replace('04Jul','07').replace('05Aug','08').replace('06Sep','09').replace('07Oct','10').replace('08Nov','11').replace('09Dec','12'))\n",
    "    date_frame['Year FY']=pd.to_numeric(date_frame['Year FY'], errors='coerce')\n",
    "    date_frame.loc[date_frame['Month FY'].isin(['01','02','03']), 'Year FY'] +=1\n",
    "    \n",
    "    def fun (row):\n",
    "        str_date = '{}.{}.{}'.format('01', row['Month FY'], row['Year FY'])\n",
    "        date = pd.datetime.strptime(str_date, '%d.%m.%Y')\n",
    "        row['DateTime'] = date\n",
    "        return row\n",
    "\n",
    "    date_frame['DateTime'] = np.NaN \n",
    "    df_with_date = date_frame.apply(fun, axis = 1)\n",
    "    return df_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Просто вариант загрузки данных, подготовки таргета и дроп переменных обджекта\n",
    "X_full = pd.read_csv('train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "X = X_full.select_dtypes(exclude=['object'])\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# простая функция сравнения  скоринга данных \n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=30, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка безопастного лейбл энкодинга в трейн-тесте\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_label_cols = [col for col in object_cols if \n",
    "                   set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be label encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверка количества уникальных значений в категорийных колонках\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "\n",
    "sorted(d.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Колонки которые можно уанхотить\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columns которые или дропать или хз \n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример пайплайна с импьютерами\n",
    "\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Собираем 2 предпроцессинга в 1 \n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример чтения данных, с дропом миссингов в таргетеи оставление ток численных колоных для бейзлайн\n",
    "train_data = pd.read_csv('../input/train.csv', index_col='Id')\n",
    "test_data = pd.read_csv('../input/test.csv', index_col='Id')\n",
    "\n",
    "\n",
    "train_data.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = train_data.SalePrice              \n",
    "train_data.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "numeric_cols = [cname for cname in train_data.columns if train_data[cname].dtype in ['int64', 'float64']]\n",
    "X = train_data[numeric_cols].copy()\n",
    "X_test = test_data[numeric_cols].copy("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Скор от итераций с построением графика \n",
    "def get_score(n_estimators):\n",
    "    my_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', SimpleImputer()),\n",
    "        ('model', RandomForestRegressor(n_estimators, random_state=0))\n",
    "    ])\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                                  cv=3,\n",
    "                                  scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "results = {}\n",
    "for i in range(1,9):\n",
    "    results[50*i] = get_score(50*i)\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results.keys(), results.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#дата\n",
    "ks = ks.assign(hour=ks.launched.dt.hour,\n",
    "               day=ks.launched.dt.day,\n",
    "               month=ks.launched.dt.month,\n",
    "               year=ks.launched.dt.year)\n",
    "\n",
    "ks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Дроп через кьри и эссайн через Труфолс в инт\n",
    "\n",
    "ks = ks.query('state != \"live\"')\n",
    "\n",
    "\n",
    "ks = ks.assign(outcome=(ks['state'] == 'successful').astype(int))\n",
    "data = ks[['goal', 'hour', 'day', 'month', 'year', 'outcome']].join(encoded)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разбивка таймсериас на трейн тест валид\n",
    "valid_fraction = 0.1\n",
    "valid_size = int(len(data) * valid_fraction)\n",
    "\n",
    "train = data[:-2 * valid_size]\n",
    "valid = data[-2 * valid_size:-valid_size]\n",
    "test = data[-valid_size:]\n",
    "#Проверка распределения целевой в трейнтествалиж\n",
    "for each in [train, valid, test]:\n",
    "    print(f\"Outcome fraction = {each.outcome.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция для экономии памяти, подходит если у вас есть большие датафреймы . В процессе оптимизации..\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция отображения пропущенных значений\n",
    "def missing_values_table(df):\n",
    "        # итого\n",
    "        mis_val = df.isnull().sum()\n",
    "        \n",
    "        # процент\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "        # табличка\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "        # ренейм\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "        # сортировка\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "        \n",
    "        # вывод\n",
    "        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        \n",
    "        # Return the dataframe with missing information\n",
    "        return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Другая функция поиска пропущенных значений\n",
    "for col in train_df.columns:\n",
    "    print(\"{0}, num. NA's: {1}\".format(col, pd.isnull(train_df[col]).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сигмоидс :D\n",
    "def sigmoid(x):\n",
    "    return 1. / (1 + np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример селектов из базы\n",
    "pg_engine_url = 'postgresql://argo:lfyystfhuj@10.1.108.254/raw_argo'\n",
    "engine = create_engine(pg_engine_url)\n",
    "df=pd.read_sql_query(\"select * from agg_argo_basic where brand = 'Actovegin' and area = 'South' and region != 'Russia BU'\", \n",
    "                     con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример работы с функциями оптиитзации. Нахождение локальных минмумов и корней квадратных уровней, можно использовать\n",
    "# и для оптимизации чего-то сложного\n",
    "f = lambda x: norm(x) ** 2 + 1\n",
    "der_f = lambda x: 2 * x\n",
    "\n",
    "res = minimize(f, [-10.0, 10.0], jac = der_f, method = 'SLSQP')\n",
    "\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "f = lambda x: (x ** 2 - 10) * (x + 2) ** 2\n",
    "x = np.linspace(-5, 5)\n",
    "\n",
    "plt.plot(x, f(x))\n",
    "plt.xlim((-5, 5))\n",
    "plt.show()\n",
    "\n",
    "res = minimize_scalar(f, method = 'brent')\n",
    "res\n",
    "\n",
    "from scipy.optimize import root\n",
    "\n",
    "f = lambda x: (np.sin(x) + 1) * (x ** 2 + 1)\n",
    "x = np.linspace(-3, 2)\n",
    "\n",
    "plt.plot(x, f(x))\n",
    "plt.xlim((-3, 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Токенайзер самописный  \n",
    "\n",
    "letters = re.compile(r'[^A-Z ]+')\n",
    "spaces = re.compile('  +')\n",
    "\n",
    "def tokenize(row):\n",
    "    row = letters.sub('', row)\n",
    "    row = spaces.sub(' ', row)\n",
    "    return row\n",
    "\n",
    "print(tokenize('PAPER CRAFT , LITTLE BIRDIE'))\n",
    "print(tokenize('????damages????'))\n",
    "print(tokenize('CREAM HANGING HEART T-LIGHT HOLDER'))\n",
    "df['DescriptionCleaned'] = df['Description'].replace(np.nan, '').apply(tokenize)\n",
    "len(df['DescriptionCleaned'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#простейший пример энкодера\n",
    "le = LabelEncoder()\n",
    "print(le.fit([\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]))\n",
    "print(le.classes_)\n",
    "print(le.transform([\"tokyo\", \"tokyo\", \"paris\"]))\n",
    "print(le.inverse_transform([2, 2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На входе словарь и лист фичей из него. Фиче через get_dummies конкат к датафрему словаря\n",
    "def create_df(dic, feature_list):\n",
    "    out = pd.DataFrame(dic)\n",
    "    out = pd.concat([out, pd.get_dummies(out[feature_list])], axis = 1)\n",
    "    out.drop(feature_list, axis = 1, inplace = True)\n",
    "    return out\n",
    "\n",
    "# Проверка что в трейне и тесте. Некоторые значения признаков есть в тесте, но нет в трейне и наоборот\n",
    "def intersect_features(train, test):\n",
    "    f = list( set(train.keys()) & set(test.keys()))\n",
    "    return train[f], test[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Важность признаком через алгоритм PermutationImportance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)\n",
    "eli5.show_weights(perm, feature_names = val_X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Чтение дропинг для бейзлайна \n",
    "X_full = pd.read_csv('train.csv', index_col='Id')\n",
    "X_test_full = pd.read_csv('test.csv', index_col='Id')\n",
    "\n",
    "# Remove rows with missing target, separate target from predictors\n",
    "X_full.dropna(axis=0, subset=['SalePrice'], inplace=True)\n",
    "y = X_full.SalePrice\n",
    "X_full.drop(['SalePrice'], axis=1, inplace=True)\n",
    "\n",
    "# To keep things simple, we'll use only numerical predictors\n",
    "X = X_full.select_dtypes(exclude=['object'])\n",
    "X_test = X_test_full.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Break off validation set from training data\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n",
    "                                                      random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the count encoder\n",
    "count_enc = ce.CountEncoder(cols=cat_features)\n",
    "\n",
    "# Learn encoding from the training set\n",
    "count_encoded=count_enc.fit(train[cat_features])\n",
    "\n",
    "# Apply encoding to the train and validation sets as new columns\n",
    "# Make sure to add `_count` as a suffix to the new columns\n",
    "train_encoded = train.join(count_encoded.transform(train[cat_features]).add_suffix(\"_count\"))\n",
    "valid_encoded = valid.join(count_encoded.transform(valid[cat_features]).add_suffix(\"_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Датафрейм подсчета соревнований за последние 7 дней\n",
    "count_7_days = launched.rolling('7d').count() - 1\n",
    "print(count_7_days.head(20))\n",
    "\n",
    "plt.plot(count_7_days[7:]);\n",
    "plt.title(\"Competitions in the last 7 days\")\n",
    "count_7_days.index = launched.values\n",
    "count_7_days = count_7_days.reindex(ks.index)\n",
    "baseline_data.join(count_7_days).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерация фичей через итертулс\n",
    "import itertools\n",
    "\n",
    "cat_features = ['ip', 'app', 'device', 'os', 'channel']\n",
    "interactions = pd.DataFrame(index=clicks.index)\n",
    "\n",
    "# Iterate through each pair of features, combine them into interaction features\n",
    "for col1, col2 in itertools.combinations(cat_features, 2):\n",
    "        new_col_name = '_'.join([col1, col2])\n",
    "\n",
    "        # Convert to strings and combine\n",
    "        new_values = clicks[col1].map(str) + \"_\" + clicks[col2].map(str)\n",
    "\n",
    "        encoder = preprocessing.LabelEncoder()\n",
    "        interactions[new_col_name] = encoder.fit_transform(new_values)\n",
    "\n",
    "# Check your answer\n",
    "q_1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#подсчет прошлый событий\n",
    "def count_past_events(series, time_window='6H'):\n",
    "        series = pd.Series(series.index, index=series)\n",
    "        # Subtract 1 so the current event isn't counted\n",
    "        past_events = series.rolling(time_window).count() - 1\n",
    "        return past_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сумма по предыдущим событиям\n",
    "def previous_attributions(series):\n",
    "        # Subtracting raw values so I don't count the current event\n",
    "        sums = series.expanding(min_periods=2).sum() - series\n",
    "        return sums\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подсчет количества слов в описании\n",
    "n_trop = reviews.description.map(lambda desc: \"tropical\" in desc).sum()\n",
    "n_fruity = reviews.description.map(lambda desc: \"fruity\" in desc).sum()\n",
    "descriptor_counts = pd.Series([n_trop, n_fruity], index=['tropical', 'fruity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#применение лямбды\n",
    "def stars(row):\n",
    "    if row.country == 'Canada':\n",
    "        return 3\n",
    "    elif row.points >= 95:\n",
    "        return 3\n",
    "    elif row.points >= 85:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "star_ratings = reviews.apply(stars, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timedeltas = clicks.groupby('ip')['click_time'].transform(time_diff)\n",
    "def time_diff(series):\n",
    "            return series.diff().dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)) : \n",
    "    if df.loc[i, \"child_id\"] == 17:\n",
    "        df.loc[i, \"name\"] = 'Показатель по Москве'\n",
    "    elif df.loc[i, \"child_id\"] == 92:\n",
    "        df.loc[i, \"name\"] = 'Показатель по Немоскве'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col3'] = df['col1'].apply(lambda x: mat[x] if x in mat else None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Все, что связано с обучением алгоритмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Простеший пример подбора параметров через GS для дерева\n",
    "tree_params = {'criterion': ('gini', 'entropy'), \n",
    "               'max_depth': list(range(1,11)), \n",
    "               'min_samples_leaf': list(range(1,11))}\n",
    "\n",
    "locally_best_tree = GridSearchCV(DecisionTreeClassifier(random_state=42), \n",
    "                                 tree_params, \n",
    "                                 verbose=True, n_jobs=1, cv=5)\n",
    "locally_best_tree.fit(train_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Лайтгбм\n",
    "import lightgbm as lgb\n",
    "\n",
    "feature_cols = train.columns.drop('outcome')\n",
    "\n",
    "dtrain = lgb.Dataset(train[feature_cols], label=train['outcome'])\n",
    "dvalid = lgb.Dataset(valid[feature_cols], label=valid['outcome'])\n",
    "\n",
    "param = {'num_leaves': 64, 'objective': 'binary'}\n",
    "param['metric'] = 'auc'\n",
    "num_round = 1000\n",
    "bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], early_stopping_rounds=10, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "feature_cols = clicks.columns.drop(['click_time', 'attributed_time', 'is_attributed'])\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    " # Do feature extraction on the training data only!\n",
    "selector = SelectKBest(f_classif, k=40)\n",
    "X_new = selector.fit_transform(train[feature_cols], train['is_attributed'])\n",
    "\n",
    "    # Get back the features we've kept, zero out all other features\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                    index=train.index, \n",
    "                                    columns=feature_cols)\n",
    "\n",
    "    # Dropped columns have values of all 0s, so var is 0, drop them\n",
    "dropped_columns = selected_features.columns[selected_features.var() == 0]\n",
    "\n",
    "# Check your answer\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def select_features_l1(X, y):\n",
    "        logistic = LogisticRegression(C=0.1, penalty=\"l1\", random_state=7).fit(X, y)\n",
    "        model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "        X_new = model.transform(X)\n",
    "\n",
    "        # Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "        selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                        index=X.index,\n",
    "                                        columns=X.columns)\n",
    "\n",
    "        # Dropped columns have values of all 0s, keep other columns \n",
    "        cols_to_keep = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "        return cols_to_keep\n",
    "\n",
    "# Check your answer\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "    \"\"\" делит датафрейм таймсериас на трейн тест и валидейшн согласн процентам от валифракшн\n",
    "    на забывайте указать поле сортировки времени. в моем случае это кликтайм. количество секция тоже можно менять\n",
    "    с двойки на другое.\n",
    "    \"\"\"\n",
    "\n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    \n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def train_model(train, valid, test=None, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    num_round = 1000\n",
    "    print(\"Training model!\")\n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    if test is not None: \n",
    "        test_pred = bst.predict(test[feature_cols])\n",
    "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "        return bst, valid_score, test_score\n",
    "    else:\n",
    "        return bst, valid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(n_estimators):\n",
    "    my_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', SimpleImputer()),\n",
    "        ('model', RandomForestRegressor(n_estimators, random_state=0))\n",
    "    ])\n",
    "    scores = -1 * cross_val_score(my_pipeline, X, y,\n",
    "                                  cv=3,\n",
    "                                  scoring='neg_mean_absolute_error')\n",
    "    return scores.mean()\n",
    "results = {}\n",
    "for i in range(1,9):\n",
    "    results[50*i] = get_score(50*i)\n",
    "# Check your answer\n",
    "step_2.check()\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(results.keys(), results.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Простеший пример подбора параметров через GS для леса\n",
    "forest_params = {'n_estimators': [100, 500],\n",
    "               'max_depth': list(range(1,5)), \n",
    "               'min_samples_leaf': list(range(1,5))}\n",
    "\n",
    "locally_best_forest = GridSearchCV(RandomForestClassifier(random_state=42), \n",
    "                                 forest_params, \n",
    "                                 verbose=True, n_jobs=-1, cv=5)\n",
    "locally_best_forest.fit(train_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Работа с CV на холдаутах\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(train_df, y, test_size=0.3, \n",
    "                                                          stratify=y, random_state=42)\n",
    "#Задаем параметры для CV + две строки для скоров\n",
    "%%time\n",
    "tree_params = {'max_depth': list(range(4,9))}\n",
    "cv_scores, holdout_scores = [], []\n",
    "# Обучаем модель на разном количестве фолдов, для каждого фолда подбираем через GS лучшие параметры.\n",
    "for k_folds in log_progress(range(2, 13)):\n",
    "    folds = StratifiedKFold(y_train, n_folds=k_folds, shuffle=True, random_state=42)\n",
    "    locally_best_tree = GridSearchCV(DecisionTreeClassifier(random_state=42), \n",
    "                                 tree_params, \n",
    "                                 verbose=False, n_jobs=-1, cv=folds,\n",
    "                                scoring='roc_auc')\n",
    "        \n",
    "    locally_best_tree.fit(X_train, y_train)\n",
    "        \n",
    "    \n",
    "    cv_scores.append(locally_best_tree.best_score_)\n",
    "    \n",
    "    #Затем скорим полученными алогритмами наш холдаут, для контроля качества.\n",
    "    holdout_scores.append(roc_auc_score(y_holdout, locally_best_tree.predict_proba(X_holdout)[:,1]))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Работа с CV на холдаутах при классифаере+ график\n",
    "%%time\n",
    "cv_scores, holdout_scores = [], []\n",
    "knn_params = {'n_neighbors': range(50, 150, 4)}\n",
    "for k_folds in log_progress(range(2, 13)):\n",
    "    folds = StratifiedKFold(y_train, n_folds=k_folds, shuffle=True, random_state=42)\n",
    "    locally_best_knn = GridSearchCV(KNeighborsClassifier(n_jobs=-1), \n",
    "                                     knn_params, \n",
    "                                     verbose=False, n_jobs=-1, cv=folds,\n",
    "                                    scoring='roc_auc')\n",
    "        \n",
    "    locally_best_knn.fit(X_train, y_train)\n",
    "    cv_scores.append(locally_best_knn.best_score_)\n",
    "    holdout_scores.append(roc_auc_score(y_holdout, locally_best_knn.predict_proba(X_holdout)[:,1]))\n",
    "\n",
    "plot(range(2, 13), cv_scores, label='cv scores')\n",
    "plot(range(2, 13), holdout_scores, label='holdout scores')\n",
    "plot(range(2, 13), [0.6123] * 11, label='LB score')\n",
    "xlabel('#folds')\n",
    "ylabel('ROC AUC')\n",
    "legend(loc='lower right')\n",
    "\n",
    "%%time\n",
    "n_neighbs = range(50, 150, 4)\n",
    "val_train, val_test = validation_curve(KNeighborsClassifier(n_jobs=-1), X_train, y_train,\n",
    "                                       'n_neighbors', \n",
    "                                       n_neighbs, cv=5,\n",
    "                                       scoring='roc_auc')\n",
    "\n",
    "def plot_learning_curve_knn(n_neighbs=100):\n",
    "    train_sizes = np.linspace(0.05, 1, 20)\n",
    "    N_train, val_train, val_test = learning_curve(KNeighborsClassifier(n_jobs=-1,\n",
    "                                                                      n_neighbors=n_neighbs),\n",
    "                                                  X_train, y_train, train_sizes, cv=3,\n",
    "                                                  scoring='roc_auc')\n",
    "    plot_with_err(N_train, val_train, label='training scores')\n",
    "    plot_with_err(N_train, val_test, label='validation scores')\n",
    "    xlabel('Training Set Size')\n",
    "    ylabel('ROC AUC')\n",
    "    legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Так лучше выделять отдельно признаки. + встроеные график важности признаков\n",
    "features_to_use = list(set(train.columns) \n",
    "                       - set(['Id', 'Cover_Type']))\n",
    "first_forest = RandomForestClassifier(bootstrap=True, \n",
    "                                      oob_score=True, \n",
    "                                      random_state=42)\n",
    "first_forest.fit(train[features_to_use], \n",
    "                 train['Cover_Type'])\n",
    "first_forest_predictions = first_forest.predict(test[features_to_use])\n",
    "pd.DataFrame(first_forest.feature_importances_,\n",
    "             index=train[features_to_use].columns).sort([0], ascending=False)[:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# простейшие компоненты после шкалирования\n",
    "girl_params = scale(girl_params) \n",
    "X = PCA(n_components=2).fit_transform(girl_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Выделение аутлаеров через SVM\n",
    "svm_clf = svm.OneClassSVM(kernel=\"rbf\")\n",
    "svm_clf.fit(X)\n",
    "dist_to_border = svm_clf.decision_function(X).ravel()\n",
    "threshold = stats.scoreatpercentile(dist_to_border,\n",
    "            100 * OUTLIER_FRACTION)\n",
    "is_outlier = dist_to_border < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Простейшая рекомендалка ( не советую использовать, есть варианты лучше , это старая версия)\n",
    "R_df = ratings_df.pivot(index = 'UserID', columns ='MovieID', values = 'Rating').fillna(0)\n",
    "R_df.head()\n",
    "R = R_df.as_matrix()\n",
    "user_ratings_mean = np.mean(R, axis = 1)\n",
    "R_demeaned = R - user_ratings_mean.reshape(-1, 1)\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "U, sigma, Vt = svds(R_demeaned, k = 50)\n",
    "\n",
    "sigma = np.diag(sigma)\n",
    "all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) + user_ratings_mean.reshape(-1, 1)\n",
    "preds_df = pd.DataFrame(all_user_predicted_ratings, columns = R_df.columns)\n",
    "preds_df.head()\n",
    "\n",
    "def recommend_movies(predictions_df, userID, movies_df, original_ratings_df, num_recommendations=5):\n",
    "    \n",
    "    # Get and sort the user's predictions\n",
    "    user_row_number = userID - 1 # UserID starts at 1, not 0\n",
    "    sorted_user_predictions = preds_df.iloc[user_row_number].sort_values(ascending=False) # UserID starts at 1\n",
    "    \n",
    "    # Get the user's data and merge in the movie information.\n",
    "    user_data = original_ratings_df[original_ratings_df.UserID == (userID)]\n",
    "    user_full = (user_data.merge(movies_df, how = 'left', left_on = 'MovieID', right_on = 'MovieID').\n",
    "                     sort_values(['Rating'], ascending=False)\n",
    "                 )\n",
    "\n",
    "    print 'User {0} has already rated {1} movies.'.format(userID, user_full.shape[0])\n",
    "    print 'Recommending highest {0} predicted ratings movies not already rated.'.format(num_recommendations)\n",
    "    \n",
    "    # Recommend the highest predicted rating movies that the user hasn't seen yet.\n",
    "    recommendations = (movies_df[~movies_df['MovieID'].isin(user_full['MovieID'])].\n",
    "         merge(pd.DataFrame(sorted_user_predictions).reset_index(), how = 'left',\n",
    "               left_on = 'MovieID',\n",
    "               right_on = 'MovieID').\n",
    "         rename(columns = {user_row_number: 'Predictions'}).\n",
    "         sort_values('Predictions', ascending = False).\n",
    "                       iloc[:num_recommendations, :-1]\n",
    "                      )\n",
    "\n",
    "    return user_full, recommendations\n",
    "already_rated, predictions = recommend_movies(preds_df, 837, movies_df, ratings_df, 10)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример решения линейной регресии ( иногда помогает в качестве первого уровня модели)\n",
    "a, b = 5, 7 # Regression parameters\n",
    "\n",
    "X = np.random.random(size=(50, 1))\n",
    "# Linear model: Y = aX + b + eps, where eps ~ N(0, 0.5)\n",
    "Y = a * X.squeeze() + b + np.random.normal(scale = 0.5, size=50) \n",
    "\n",
    "# fit_intercept stands for 'b' in the equation above\n",
    "model = LinearRegression(fit_intercept=True) \n",
    "model.fit(X, Y)\n",
    "\n",
    "print (\"Y = %.3fX + %.3f + eps\" % (model.coef_, model.intercept_))\n",
    "\n",
    "# Plotting the data and the model prediction\n",
    "X_test = np.linspace(0, 1, 100)\n",
    "\n",
    "plt.plot(X_test.squeeze(), model.predict(X_test[:, np.newaxis]), \n",
    "         linewidth = 4, color =  'k');\n",
    "plt.plot(X.squeeze(), Y, 'ro', ms = 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Простейшая кластеризация\n",
    "k_means = KMeans(n_clusters = 3)\n",
    "y = k_means.fit_predict(X_reduced)\n",
    "\n",
    "plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c = y, s = 70, cmap='autumn')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример графиков для отслеживания , например MSE  рукописный ( есть встроенные)\n",
    "X_train, y_train = generate(n_samples=n_train, noise=noise)\n",
    "X_test, y_test = generate(n_samples=n_test, noise=noise)\n",
    "\n",
    "# One decision tree regressor\n",
    "dtree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "d_predict = dtree.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, d_predict, \"g\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Decision tree regressor, MSE = %.2f\" \n",
    "          % np.sum((y_test - d_predict) ** 2))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=50, \n",
    "                           min_samples_leaf=3).fit(X_train, y_train)\n",
    "rf_predict = rf.predict(X_test)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X_test, f(X_test), \"b\")\n",
    "plt.scatter(X_train, y_train, c=\"b\", s=20)\n",
    "plt.plot(X_test, rf_predict, \"r\", lw=2)\n",
    "plt.xlim([-5, 5])\n",
    "plt.title(\"Random forest regressor, MSE = %.2f\" % np.sum((y_test - rf_predict) ** 2));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# рекомендалка посложнее\n",
    "# Словарь кинокритиков и выставленных ими оценок для небольшого набора\n",
    "# данных о фильмах\n",
    "critics={'Lisa Rose': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.5,\n",
    " 'Just My Luck': 3.0, 'Superman Returns': 3.5, 'You, Me and Dupree': 2.5, \n",
    " 'The Night Listener': 3.0},\n",
    "'Gene Seymour': {'Lady in the Water': 3.0, 'Snakes on a Plane': 3.5, \n",
    " 'Just My Luck': 1.5, 'Superman Returns': 5.0, 'The Night Listener': 3.0, \n",
    " 'You, Me and Dupree': 3.5},\n",
    "'Michael Phillips': {'Lady in the Water': 2.5, 'Snakes on a Plane': 3.0,\n",
    " 'Superman Returns': 3.5, 'The Night Listener': 4.0},\n",
    "'Claudia Puig': {'Snakes on a Plane': 3.5, 'Just My Luck': 3.0,\n",
    " 'The Night Listener': 4.5, 'Superman Returns': 4.0, \n",
    " 'You, Me and Dupree': 2.5},\n",
    "'Mick LaSalle': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0, \n",
    " 'Just My Luck': 2.0, 'Superman Returns': 3.0, 'The Night Listener': 3.0,\n",
    " 'You, Me and Dupree': 2.0}, \n",
    "'Jack Matthews': {'Lady in the Water': 3.0, 'Snakes on a Plane': 4.0,\n",
    " 'The Night Listener': 3.0, 'Superman Returns': 5.0, 'You, Me and Dupree': 3.5},\n",
    "'Toby': {'Snakes on a Plane':4.5,'You, Me and Dupree':1.0,'Superman Returns':4.0}}\n",
    "# Получить рекомендации для заданного человека, пользуясь взвешенным средним\n",
    "# оценок, данных всеми остальными пользователями\n",
    "def getRecommendations(prefs,person,similarity=sim_pearson):\n",
    "    totals={}\n",
    "    simSums={}\n",
    "    for other in prefs:\n",
    "    # сравнивать меня с собой же не нужно\n",
    "        if other==person: continue\n",
    "        sim=similarity(prefs,person,other)\n",
    "    # игнорировать нулевые и отрицательные оценки\n",
    "        if sim<=0: continue\n",
    "        for item in prefs[other]:\n",
    "    # оценивать только фильмы, которые я еще не смотрел\n",
    "            if item not in prefs[person] or prefs[person][item]==0:\n",
    "    # Коэффициент подобия * Оценка\n",
    "                totals.setdefault(item,0)\n",
    "                totals[item]+=prefs[other][item]*sim\n",
    "        # Сумма коэффициентов подобия\n",
    "                simSums.setdefault(item,0)\n",
    "                simSums[item]+=sim\n",
    "    # Создать нормированный список\n",
    "    rankings=[(total/simSums[item],item) for item,total in totals.items( )]\n",
    "    # Вернуть отсортированный список\n",
    "    rankings.sort( )\n",
    "    rankings.reverse( )\n",
    "    return rankings\n",
    "def transformPrefs(prefs):\n",
    "    result={}\n",
    "    for person in prefs:\n",
    "        for item in prefs[person]:\n",
    "            result.setdefault(item,{})\n",
    "    # Обменять местами человека и предмет\n",
    "            result[item][person]=prefs[person][item]\n",
    "    return result\n",
    "getRecommendations(movies,'Just My Luck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# продолжение предыдущих рекомендалок. В целом этого хватает на самый простой бейзлан за 2 дня. \n",
    "def topMatches(prefs,person,n=5,similarity=sim_pearson):\n",
    "    scores=[(similarity(prefs,person,other),other)\n",
    "    for other in prefs if other!=person]\n",
    "    # Отсортировать список по убыванию оценок\n",
    "    scores.sort( )\n",
    "    scores.reverse( )\n",
    "    return scores[0:n]\n",
    "def calculateSimilarItems(prefs,n=10):\n",
    "    # Создать словарь, содержащий для каждого образца те образцы, которые\n",
    "    # больше всего похожи на него.\n",
    "    result={}\n",
    "    # Обратить матрицу предпочтений, чтобы строки соответствовали образцам\n",
    "    itemPrefs=transformPrefs(prefs)\n",
    "    c=0\n",
    "    for item in itemPrefs:\n",
    "    # Обновление состояния для больших наборов данных\n",
    "        c+=1\n",
    "        if c%100==0: print \"%d / %d\" % (c,len(itemPrefs))\n",
    "    # Найти образцы, максимально похожие на данный\n",
    "        scores=topMatches(itemPrefs,item,n=n,similarity=sim_distance)\n",
    "        result[item]=scores\n",
    "    return result\n",
    "def getRecommendedItems(prefs,itemMatch,user):\n",
    "    userRatings=prefs[user]\n",
    "    scores={}\n",
    "    totalSim={}\n",
    "\n",
    "    # Цикл по образцам, оцененным данным пользователем\n",
    "    for (item,rating) in userRatings.items():\n",
    "        \n",
    "    # Цикл по образцам, похожим на данный\n",
    "        for (similarity,item2) in itemMatch[item]:\n",
    "    # Пропускаем, если пользователь уже оценивал данный образец\n",
    "            if item2 in userRatings: continue\n",
    "        # Взвешенная суммы оценок, умноженных на коэффициент подобия\n",
    "            scores.setdefault(item2,0)\n",
    "            scores[item2]+=similarity*rating\n",
    "        # Сумма всех коэффициентов подобия\n",
    "            totalSim.setdefault(item2,0)\n",
    "            totalSim[item2]+=similarity\n",
    "\n",
    "    # Делим каждую итоговую оценку на взвешенную сумму, чтобы вычислить\n",
    "    # среднее\n",
    "    rankings=[(score/totalSim[item],item) for item,score in scores.items( )]\n",
    "\n",
    "    # Возвращает список rankings, отсортированный по убыванию\n",
    "    rankings.sort( )\n",
    "    rankings.reverse( )\n",
    "    return rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Fitting 3 folds for each of 9 candidates, totalling 27 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6001029721598573\n",
      "{'max_depth': 4, 'n_estimators': 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    1.4s finished\n"
     ]
    }
   ],
   "source": [
    "#Пример использования early_stopping_rounds , важно, чтобы долго не обучать\n",
    "X = boston['data']\n",
    "y = boston['target']\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_grid = GridSearchCV(xgb_model,\n",
    "                   {'max_depth': [2,4,6],\n",
    "                    'n_estimators': [50,100,200]}, verbose=1)\n",
    "xgb_grid.fit(X,y)\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits['data']\n",
    "y = digits['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "clf = xgb.XGBClassifier()\n",
    "clf.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"merror\",\n",
    "        eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Простейшая нейросеть на MNIST\n",
    "\n",
    "# Устанавливаем seed для повторяемости результатов\n",
    "numpy.random.seed(42)\n",
    "\n",
    "# Загружаем данные\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Преобразование размерности изображений\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "# Нормализация данных\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Преобразуем метки в категории\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Создаем последовательную модель\n",
    "model = Sequential()\n",
    "\n",
    "# Добавляем уровни сети\n",
    "model.add(Dense(800, input_dim=784, init=\"normal\", activation=\"relu\"))\n",
    "model.add(Dense(10, init=\"normal\", activation=\"softmax\"))\n",
    "\n",
    "# Компилируем модель\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# Обучаем сеть\n",
    "model.fit(X_train, Y_train, batch_size=200, nb_epoch=100, validation_split=0.2, verbose=1)\n",
    "\n",
    "# Оцениваем качество обучения сети на тестовых данных\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Точность работы на тестовых данных: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "model.add(Dense(800, input_dim=784, init=\"normal\", activation=\"relu\"))\n",
    "model.add(Dense(600, init=\"normal\", activation=\"relu\"))\n",
    "model.add(Dense(10, init=\"normal\", activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Простейшая нейросеть для CIFAR. (В целом любая простейшая нейросеть тут служит проверкой на гипотезу Решаемости задачи.)\n",
    "\n",
    "# Задаем seed для повторяемости результатов\n",
    "numpy.random.seed(42)\n",
    "\n",
    "# Загружаем данные\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Размер мини-выборки\n",
    "batch_size = 32\n",
    "# Количество классов изображений\n",
    "nb_classes = 10\n",
    "# Количество эпох для обучения\n",
    "nb_epoch = 25\n",
    "# Размер изображений\n",
    "img_rows, img_cols = 32, 32\n",
    "# Количество каналов в изображении: RGB\n",
    "img_channels = 3\n",
    "\n",
    "# Нормализуем данные\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Преобразуем метки в категории\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "# Создаем последовательную модель\n",
    "model = Sequential()\n",
    "# Первый сверточный слой\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same',\n",
    "                        input_shape=(3, 32, 32), activation='relu'))\n",
    "# Второй сверточный слой\n",
    "model.add(Convolution2D(32, 3, 3, activation='relu'))\n",
    "# Первый слой подвыборки\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Слой регуляризации Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Третий сверточный слой\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same', activation='relu'))\n",
    "# Четвертый сверточный слой\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "# Второй слой подвыборки\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "# Слой регуляризации Dropout\n",
    "model.add(Dropout(0.25))\n",
    "# Слой преобразования данных из 2D представления в плоское\n",
    "model.add(Flatten())\n",
    "# Полносвязный слой для классификации\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# Слой регуляризации Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Выходной полносвязный слой\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# Задаем параметры оптимизации\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy'])\n",
    "# Обучаем модель\n",
    "model.fit(X_train, Y_train,\n",
    "              batch_size=batch_size,\n",
    "              nb_epoch=nb_epoch,\n",
    "              validation_split=0.1,\n",
    "              shuffle=True)\n",
    "\n",
    "# Оцениваем качество обучения модели на тестовых данных\n",
    "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Точность работы на тестовых данных: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пример работы с каунтером и векторазером\n",
    "from collections import Counter\n",
    "docs = []\n",
    "word_count = Counter()\n",
    "descs_unique = pd.DataFrame(df['DescriptionCleaned'].unique(), columns=['DescriptionCleaned'])\n",
    "for idx, row in descs_unique.iterrows():\n",
    "    desc = row['DescriptionCleaned']\n",
    "    tokens = filter(None, desc.split(' '))\n",
    "    docs.append(Counter(tokens))\n",
    "    for t in tokens:\n",
    "        \n",
    "        word_count[t] += 1\n",
    "\n",
    "words_most = dict(word_count.most_common(300))\n",
    "words_most\n",
    "docs_filtered = [{k: v for k, v in d.iteritems() if k in words_most} for d in docs]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(docs_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подбор количества кластеров для кминс через силуэтную меру\n",
    "inertias = []\n",
    "silhs = []\n",
    "for n_clusters in range(3, 15):\n",
    "    \n",
    "    print(n_clusters)\n",
    "    \n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=30)\n",
    "    kmeans.fit(X)\n",
    "    clusters = kmeans.predict(X)\n",
    "    silhouette_avg = silhouette_score(X, clusters)\n",
    "    \n",
    "    print(\"For n_clusters =\", n_clusters, \"The inertia_ is :\", kmeans.inertia_, \"The average silhouette_score is :\", silhouette_avg)\n",
    "    \n",
    "    inertias.append(kmeans.inertia_)\n",
    "    silhs.append(silhouette_avg)\n",
    "plt.plot(range(3, 15), inertias)\n",
    "plt.plot(range(3, 15), silhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подбор количества кластеров для кминс через силуэтную меру + доп графики\n",
    "def clusters_stats(n_clusters, clusters, print_words=True):\n",
    "    fig, ax1 = plt.subplots(1, 1)\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    silhouette_avg = silhouette_score(X, clusters)\n",
    "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, clusters)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[clusters == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    print(np.bincount(clusters))\n",
    "    if print_words:\n",
    "        cluster_words_count = defaultdict(Counter)\n",
    "\n",
    "        for c, doc in zip(clusters, docs_filtered):\n",
    "            for w in doc.keys():\n",
    "                cluster_words_count[c][w] += 1\n",
    "\n",
    "        for c in cluster_words_count:\n",
    "            print('Cluster {}'.format(c))\n",
    "            print(', '.join('{}: {}'.format(w, count) for w, count in cluster_words_count[c].most_common(10)))\n",
    "            print()\n",
    "n_clusters = 5\n",
    "kmeans = KMeans(init='k-means++', n_clusters=n_clusters)\n",
    "kmeans.fit(X)\n",
    "clusters = kmeans.predict(X)\n",
    "kmeans.inertia_\n",
    "clusters_stats(n_clusters, clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Полезный профайлинг - всем советую\n",
    "pandas_profiling.ProfileReport(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Писать не через пайплайн - преступление против кода. \n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, Imputer\n",
    "\n",
    "def get_num_cols(df):\n",
    "    return df[['Age', 'Fare']]\n",
    "\n",
    "pipeline = make_union(*[\n",
    "    make_pipeline(FunctionTransformer(get_num_cols, validate=False), Imputer(strategy='mean'), MinMaxScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Писать не через пайплайн - преступление против кода. \n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, Imputer\n",
    "\n",
    "def get_num_cols(df):\n",
    "    return df[['Age', 'Fare']]\n",
    "\n",
    "pipeline = make_union(*[\n",
    "    make_pipeline(FunctionTransformer(get_num_cols, validate=False), Imputer(strategy='mean'), StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Писать не через пайплайн - преступление против кода. Тут переписанный скейлер, который работает в пайплайне, встроенные багует\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class FeaturesSum(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        return np.sum(X, axis=1).reshape(-1, 1)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.fit(X).transform(X)\n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, Imputer\n",
    "\n",
    "def get_num_cols(df):\n",
    "    return df[['Age', 'Fare']]\n",
    "\n",
    "pipeline = make_union(*[\n",
    "    make_pipeline(FunctionTransformer(get_num_cols, validate=False), Imputer(strategy='mean'), FeaturesSum(), StandardScaler()),\n",
    "    make_pipeline(FunctionTransformer(get_num_cols, validate=False), Imputer(strategy='mean'), StandardScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Итого пример как должно получаться\n",
    "def get_sex_col(df):\n",
    "    return df[['Sex']]\n",
    "\n",
    "def get_num_cols(df):\n",
    "    return df[['Age', 'Fare']]\n",
    "\n",
    "vec = make_union(*[\n",
    "    make_pipeline(FunctionTransformer(get_sex_col, validate=False),  LabelEncoderPipelineFriendly()),\n",
    "    make_pipeline(FunctionTransformer(get_num_cols, validate=False), Imputer(strategy='mean'), FeaturesSum(), StandardScaler()),\n",
    "    make_pipeline(FunctionTransformer(get_num_cols, validate=False), Imputer(strategy='mean'), MinMaxScaler()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Анализ и процедуры кегловского соревнования. \n",
    "DATA_PATH = \"../input/ashrae-energy-prediction/\"\n",
    "train_df = pd.read_csv(DATA_PATH + 'train.csv')\n",
    "\n",
    "# Remove outliers \n",
    "train_df = train_df [ train_df['building_id'] != 1099 ]\n",
    "train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')\n",
    "\n",
    "building_df = pd.read_csv(DATA_PATH + 'building_metadata.csv')\n",
    "weather_df = pd.read_csv(DATA_PATH + 'weather_train.csv')\n",
    "def fill_weather_dataset(weather_df):\n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n",
    "    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_df = pd.concat([weather_df,new_rows])\n",
    "\n",
    "        weather_df = weather_df.reset_index(drop=True)           \n",
    "\n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    # Step 2\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "    weather_df.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather_df.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather_df.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime','day','week','month'],axis=1)\n",
    "        \n",
    "    return weather_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def features_engineering(df):\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df.sort_values(\"timestamp\")\n",
    "    df.reset_index(drop=True)\n",
    "    \n",
    "    # Add more features\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    df[\"weekend\"] = df[\"timestamp\"].dt.weekday\n",
    "    holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                    \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                    \"2017-01-02\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                    \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                    \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                    \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                    \"2019-01-01\"]\n",
    "    df[\"is_holiday\"] = (df.timestamp.isin(holidays)).astype(int)\n",
    "    df['square_feet'] =  np.log1p(df['square_feet'])\n",
    "    \n",
    "    # Remove Unused Columns\n",
    "    drop = [\"timestamp\",\"sea_level_pressure\", \"wind_direction\", \"wind_speed\",\"year_built\",\"floor_count\"]\n",
    "    df = df.drop(drop, axis=1)\n",
    "    gc.collect()\n",
    "    \n",
    "    # Encode Categorical Data\n",
    "    le = LabelEncoder()\n",
    "    df[\"primary_use\"] = le.fit_transform(df[\"primary_use\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пример LGBM\n",
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"is_holiday\", \"weekend\"]\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 1280,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.85,\n",
    "    \"reg_lambda\": 2,\n",
    "    \"metric\": \"rmse\",\n",
    "}\n",
    "\n",
    "kf = KFold(n_splits=3)\n",
    "models = []\n",
    "for train_index,test_index in kf.split(features):\n",
    "    train_features = features.loc[train_index]\n",
    "    train_target = target.loc[train_index]\n",
    "    \n",
    "    test_features = features.loc[test_index]\n",
    "    test_target = target.loc[test_index]\n",
    "    \n",
    "    d_training = lgb.Dataset(train_features, label=train_target,categorical_feature=categorical_features, free_raw_data=False)\n",
    "    d_test = lgb.Dataset(test_features, label=test_target,categorical_feature=categorical_features, free_raw_data=False)\n",
    "    \n",
    "    model = lgb.train(params, train_set=d_training, num_boost_round=1000, valid_sets=[d_training,d_test], verbose_eval=25, early_stopping_rounds=50)\n",
    "    models.append(model)\n",
    "    del train_features, train_target, test_features, test_target, d_training, d_test\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пример обучение через 50 на 50 размеры данных. ( обучаемся на 1 половине, потом на второй, результат усредняем)\n",
    "X_half_1 = X_train[:int(X_train.shape[0] / 2)]\n",
    "X_half_2 = X_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "y_half_1 = y_train[:int(X_train.shape[0] / 2)]\n",
    "y_half_2 = y_train[int(X_train.shape[0] / 2):]\n",
    "\n",
    "categorical_features = [\"building_id\", \"site_id\", \"meter\", \"primary_use\", \"hour\", \"weekday\"]\n",
    "\n",
    "d_half_1 = lgb.Dataset(X_half_1, label=y_half_1, categorical_feature=categorical_features, free_raw_data=False)\n",
    "d_half_2 = lgb.Dataset(X_half_2, label=y_half_2, categorical_feature=categorical_features, free_raw_data=False)\n",
    "\n",
    "watchlist_1 = [d_half_1, d_half_2]\n",
    "watchlist_2 = [d_half_2, d_half_1]\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"boosting\": \"gbdt\",\n",
    "    \"num_leaves\": 40,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"feature_fraction\": 0.85,\n",
    "    \"reg_lambda\": 2,\n",
    "    \"metric\": \"rmse\"\n",
    "}\n",
    "\n",
    "print(\"Building model with first half and validating on second half:\")\n",
    "model_half_1 = lgb.train(params, train_set=d_half_1, num_boost_round=1000, valid_sets=watchlist_1, verbose_eval=200, early_stopping_rounds=200)\n",
    "\n",
    "print(\"Building model with second half and validating on first half:\")\n",
    "model_half_2 = lgb.train(params, train_set=d_half_2, num_boost_round=1000, valid_sets=watchlist_2, verbose_eval=200, early_stopping_rounds=200)\n",
    "pred = np.expm1(model_half_1.predict(X_test, num_iteration=model_half_1.best_iteration)) / 2\n",
    "\n",
    "del model_half_1\n",
    "gc.collect()\n",
    "\n",
    "pred += np.expm1(model_half_2.predict(X_test, num_iteration=model_half_2.best_iteration)) / 2\n",
    "    \n",
    "del model_half_2\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Пример обучения бустинга со своей функцией ( записать mape)\n",
    "def rmsle(y_true, y_pred):\n",
    "    return 'RMSLE', np.sqrt(np.mean(np.power(np.log1p(y_pred) - np.log1p(y_true), 2))), False\n",
    "\n",
    "\n",
    "print('кстомная функция.')\n",
    "# train\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric=rmsle,\n",
    "        early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пример чтение из базы данных  + Временные ряды\n",
    "pg_engine_url = 'postgresql://argo:lfyystfhuj@10.1.108.254/raw_argo'\n",
    "engine = create_engine(pg_engine_url)\n",
    "Arima_Check=pd.read_sql_query(\" select  sum_salesrur_actual, dt  from prod.agg_argo_basic  where  brand='Actovegin' and region='Middle Volga reg'  order by dt asc \", con=engine)\n",
    "Arima_Check[\"dt\"]=pd.to_datetime(Arima_Check[\"dt\"])\n",
    "Arima_Check=Arima_Check.set_index('dt')\n",
    "#Обучение\n",
    "init_order = {tc.P_SMALL_KN: 5, tc.D_SMALL_KN: 1, tc.Q_SMALL_KN: 5}\n",
    "init_seasonal_order = {tc.P_LARGE_KN: 0, tc.D_LARGE_KN: 0, tc.Q_LARGE_KN: 0}\n",
    "arima = AutoArima(init_order= init_order ,init_seasonal_order=init_seasonal_order, seasonal= 12)\n",
    "arima.fit(data=Arima_Check,exogenous=None,test_size=6)\n",
    "#Предикшн\n",
    "end_forecast = pd.to_datetime(Arima_Check.index.values[-1]) + relativedelta(\n",
    "            months=12)\n",
    "\n",
    "arima.predict(end_forecast=end_forecast)\n",
    "\n",
    "arima.fit(data=Arima_Check,test_size=10,exogenous=None)\n",
    "arima.predict(end_forecast=72)\n",
    "#arima.fit(data, regressor, test_size)\n",
    "#arime.predict(Arima_Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример использования LE с условиями на ячейку.\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "\n",
    "# Iterate through the columns\n",
    "for col in app_train:\n",
    "    if app_train[col].dtype == 'object':\n",
    "        # If 2 or fewer unique categories\n",
    "        if len(list(app_train[col].unique())) <= 2:\n",
    "            # Train on the training data\n",
    "            le.fit(app_train[col])\n",
    "            # Transform both training and testing data\n",
    "            app_train[col] = le.transform(app_train[col])\n",
    "            app_test[col] = le.transform(app_test[col])\n",
    "            \n",
    "            # Keep track of how many columns were label encoded\n",
    "            le_count += 1\n",
    "            \n",
    "print('%d columns were label encoded.' % le_count)\n",
    "# one-hot encoding of categorical variables\n",
    "app_train = pd.get_dummies(app_train)\n",
    "app_test = pd.get_dummies(app_test)\n",
    "\n",
    "print('Training Features shape: ', app_train.shape)\n",
    "print('Testing Features shape: ', app_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Еще 1 пример GS\n",
    "mdl = lgbm.LGBMClassifier(n_estimators=50, silent=True)\n",
    "mdl.get_params().keys()\n",
    "GridParams = {\n",
    "    'learning_rate': [0.5, 0.005],\n",
    "    'n_estimators': [40, 200, 1000],\n",
    "    'num_leaves': [5, 10, 15],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary'],\n",
    "    'random_state' : [501], \n",
    "    'colsample_bytree' : [0.66],\n",
    "    'subsample' : [0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.4],\n",
    "    }\n",
    "grid = GridSearchCV(mdl, gridParams,\n",
    "                    verbose=3,\n",
    "                    cv=4,\n",
    "                    n_jobs=2)\n",
    "%time\n",
    "grid.fit(X_train, y_train)\n",
    "# CV - может 3 (через KFold, Random)\n",
    "#декар. произ. парам. получаем точки на сетки - оптимальные параметры модели\n",
    "grid = GridSearchCV(mdl, gridParams,\n",
    "                    verbose=3,\n",
    "                    cv=4,\n",
    "                    n_jobs=2)\n",
    "\n",
    "# CV - может 3 (через KFold, Random)\n",
    "grid.best_estimator_ # модель с типо крутыми параметрами\n",
    "print(grid.best_params_) # набор параметров \n",
    "print(grid.best_score_) # с наилучшей оценкой\n",
    "\n",
    "grid_best_params = grid.best_params_\n",
    "grid.grid_scores_[:10]# ср. оценка качества + оклонение + параметры с такой оценкой\n",
    "\n",
    "gbm = lgbm.LGBMClassifier(**grid.best_params_)\n",
    "\n",
    "\n",
    "gbm.fit(X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        eval_metric='l1',\n",
    "        early_stopping_rounds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Пример подбора слов синонимов\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    \"\"\"\n",
    "    Cosine similarity reflects the degree of similariy between u and v\n",
    "        \n",
    "    Arguments:\n",
    "        u -- a word vector of shape (n,)          \n",
    "        v -- a word vector of shape (n,)\n",
    "\n",
    "    Returns:\n",
    "        cosine_similarity -- the cosine similarity between u and v defined by the formula above.\n",
    "    \"\"\"\n",
    "    \n",
    "    distance = 0.0\n",
    "    \n",
    "    \n",
    "    # Compute the dot product between u and v (≈1 line)\n",
    "    dot = np.dot(u, v)\n",
    "    # Compute the L2 norm of u (≈1 line)\n",
    "    norm_u = np.sqrt(np.sum(u**2))\n",
    "    \n",
    "    # Compute the L2 norm of v (≈1 line)\n",
    "    norm_v = np.sqrt(np.sum(v**2))\n",
    "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
    "    cosine_similarity = dot / np.dot(norm_u, norm_v)\n",
    "    \n",
    "    \n",
    "    return cosine_similarity\n",
    "father = word_to_vec_map[\"father\"]\n",
    "mother = word_to_vec_map[\"mother\"]\n",
    "ball = word_to_vec_map[\"ball\"]\n",
    "crocodile = word_to_vec_map[\"crocodile\"]\n",
    "france = word_to_vec_map[\"france\"]\n",
    "italy = word_to_vec_map[\"italy\"]\n",
    "paris = word_to_vec_map[\"paris\"]\n",
    "rome = word_to_vec_map[\"rome\"]\n",
    "\n",
    "print(\"cosine_similarity(father, mother) = \", cosine_similarity(father, mother))\n",
    "print(\"cosine_similarity(ball, crocodile) = \",cosine_similarity(ball, crocodile))\n",
    "print(\"cosine_similarity(france - paris, rome - italy) = \",cosine_similarity(france - paris, rome - italy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# продолжение предыдущего\n",
    "\n",
    "def complete_analogy(word_a, word_b, word_c, word_to_vec_map):\n",
    "    \"\"\"\n",
    "    Performs the word analogy task as explained above: a is to b as c is to ____. \n",
    "    \n",
    "    Arguments:\n",
    "    word_a -- a word, string\n",
    "    word_b -- a word, string\n",
    "    word_c -- a word, string\n",
    "    word_to_vec_map -- dictionary that maps words to their corresponding vectors. \n",
    "    \n",
    "    Returns:\n",
    "    best_word --  the word such that v_b - v_a is close to v_best_word - v_c, as measured by cosine similarity\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert words to lower case\n",
    "    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)\n",
    "    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    words = word_to_vec_map.keys()\n",
    "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
    "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
    "\n",
    "    # loop over the whole word vector set\n",
    "    for w in words:        \n",
    "        # to avoid best_word being one of the input words, pass on them.\n",
    "        if w in [word_a, word_b, word_c] :\n",
    "            continue\n",
    "        \n",
    "        ### START CODE HERE ###\n",
    "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
    "        cosine_sim = cosine_similarity((e_b - e_a), (word_to_vec_map[w] - e_c))\n",
    "        \n",
    "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
    "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
    "        if cosine_sim > max_cosine_sim:\n",
    "            max_cosine_sim = cosine_sim\n",
    "            best_word = w\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    return best_word\n",
    "triads_to_try = [('italy', 'italian', 'spain'), ('india', 'delhi', 'japan'), ('man', 'woman', 'boy'), ('small', 'smaller', 'large')]\n",
    "for triad in triads_to_try:\n",
    "    print ('{} -> {} :: {} -> {}'.format( *triad, complete_analogy(*triad,word_to_vec_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# еще 1 энкодер\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n",
    "        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n",
    "        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n",
    "        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n",
    "        'YrSold', 'MoSold')\n",
    "# process columns, apply LabelEncoder to categorical features\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(all_data[c].values)) \n",
    "    all_data[c] = lbl.transform(list(all_data[c].values))\n",
    "\n",
    "# shape        \n",
    "print('Shape all_data: {}'.format(all_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пример простейшего усреднения моделей и стакинга\n",
    "n_folds = 12\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.00099,max_iter=90000, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0009, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=10000, learning_rate=0.01,\n",
    "                                   max_depth=8, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor( \n",
    "                             learning_rate=0.01, max_depth=8, \n",
    "                              n_estimators=10000,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.7571,\n",
    "                             \n",
    "                             random_state =7, nthread = -1)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',\n",
    "                              learning_rate=0.01, n_estimators=10000,\n",
    "                             )\n",
    "score = rmsle_cv(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(KRR)\n",
    "print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = rmsle_cv(model_lgb)\n",
    "print(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))\n",
    "\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)   \n",
    "averaged_models = AveragingModels(models = (ENet, KRR, lasso))\n",
    "\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))  \n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=10):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, KRR),\n",
    "                                                 meta_model = lasso)\n",
    "\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n",
    "\n",
    "stacked_averaged_models.fit(train.values, y_train)\n",
    "stacked_train_pred = stacked_averaged_models.predict(train.values)\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n",
    "print(rmsle(y_train, stacked_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример парсинга имен на английском из текста\n",
    "person_list = []\n",
    "person_names=person_list\n",
    "\n",
    "def get_human_names(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "    pos = nltk.pos_tag(tokens)\n",
    "    sentt = nltk.ne_chunk(pos, binary = False)\n",
    "\n",
    "    person = []\n",
    "    name = \"\"\n",
    "    for subtree in sentt.subtrees(filter=lambda t: t.label() == 'PERSON'):\n",
    "        for leaf in subtree.leaves():\n",
    "            person.append(leaf[0])\n",
    "        if len(person) > 1: #avoid grabbing lone surnames\n",
    "            for part in person:\n",
    "                name += part + ' '\n",
    "            if name[:-1] not in person_list:\n",
    "                person_list.append(name[:-1])\n",
    "            name = ''\n",
    "        person = []\n",
    "#     print (person_list)\n",
    "\n",
    "text = \"\"\"\n",
    "\n",
    "Some economists have responded positively to Bitcoin, including \n",
    "Francois R. Velde, senior economist of the Federal Reserve in Chicago \n",
    "who described it as \"an elegant solution to the problem of creating a \n",
    "digital currency.\" In November 2013 Richard Branson announced that \n",
    "Virgin Galactic would accept Bitcoin as payment, saying that he had invested \n",
    "in Bitcoin and found it \"fascinating how a whole new global currency \n",
    "has been created\", encouraging others to also invest in Bitcoin.\n",
    "Other economists commenting on Bitcoin have been critical. \n",
    "Economist Paul Krugman has suggested that the structure of the currency \n",
    "incentivizes hoarding and that its value derives from the expectation that \n",
    "others will accept it as payment. Economist Larry Summers has expressed \n",
    "a \"wait and see\" attitude when it comes to Bitcoin. Nick Colas, a market \n",
    "strategist for ConvergEx Group, has remarked on the effect of increasing \n",
    "use of Bitcoin and its restricted supply, noting, \"When incremental \n",
    "adoption meets relatively fixed supply, it should be no surprise that \n",
    "prices go up. And that’s exactly what is happening to BTC prices.\"\n",
    "\"\"\"\n",
    "\n",
    "names = get_human_names(text)\n",
    "for person in person_list:\n",
    "    person_split = person.split(\" \")\n",
    "    for name in person_split:\n",
    "        if wordnet.synsets(name):\n",
    "            if(name in person):\n",
    "                person_names.remove(person)\n",
    "                break\n",
    "\n",
    "print(person_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Линейная сортировка, полезна при оптимизации вместо типичного сортбай\n",
    "A = list(map(int, input().split()))\n",
    "n = max(A)\n",
    "B = [0] * (n + 1)\n",
    "for i in A:\n",
    "    B[i] = B[i] + 1\n",
    "for j in range(n + 1):\n",
    "    print((str(j) + ' ') * B[j], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#некоторые кастомные функции потерь, бывают полезны\n",
    "def fair_obj(dtrain, preds):\n",
    "    \"\"\"y = c * abs(x) - c * np.log(abs(abs(x) + c))\"\"\"\n",
    "    x = preds - dtrain\n",
    "    c = 1\n",
    "    den = abs(x) + c\n",
    "    treat = dtrain > 0\n",
    "    cont = dtrain < 0\n",
    "    \n",
    "    grad = c*x / den\n",
    "    hess = c*c / den ** 2\n",
    "    return grad, hess\n",
    "\n",
    "def huber_approx_obj(dtrain, preds):\n",
    "    d = dtrain - preds  #remove .get_labels() for sklearn\n",
    "    h = 1  #h is delta in the graphic\n",
    "    scale = 1 + (d / h) ** 2\n",
    "    scale_sqrt = np.sqrt(scale)\n",
    "    grad = d / scale_sqrt\n",
    "    hess = 1 / scale / scale_sqrt\n",
    "    return grad, hess\n",
    "\n",
    "def log_cosh_obj(dtrain, preds):\n",
    "    x = preds - dtrain\n",
    "    grad = np.tanh(x)\n",
    "    hess = 1 / np.cosh(x)**2\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пример генерации имен\n",
    "vowels_lower = 'aeiou'\n",
    "vowels_upper = vowels_lower.upper()\n",
    "consonants_lower = \"bcdfghjklmnpqrstvwxz\"\n",
    "consonants_upper = consonants_lower.upper()\n",
    "\n",
    "\n",
    "def generate_name():\n",
    "    name = random.choice(vowels_upper)\n",
    "    for i in range(random.randint(1, 2)):\n",
    "        name += random.choice(consonants_lower)\n",
    "        name += random.choice(vowels_lower)\n",
    "    return name\n",
    "\n",
    "\n",
    "def generate_surname():\n",
    "    name = random.choice(consonants_upper)\n",
    "    for i in range(random.randint(1, 4)):\n",
    "        name += random.choice(vowels_lower)\n",
    "        name += random.choice(consonants_lower)\n",
    "    return name\n",
    "\n",
    "print(generate_name())\n",
    "print(generate_surname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#Пример генерации дат и др значений. ( если нужно быстро сгенерить какие-то выборки)\n",
    "def get_date():\n",
    "    return datetime.date(year=random.randint(1950, 2018), month=random.choice((2, 9)), day=1)\n",
    "\n",
    "dates = [get_date() for _ in range(n)]\n",
    "faculty_names = ('IT', 'BUSINESS', 'MANAGEMENT', 'SCIENCE')\n",
    "faculties = [random.choice(faculty_names) for _ in range(n)]\n",
    "gender_values = ('men', 'woman', None)\n",
    "genders = [random.choice(gender_values) for i in range(n)]\n",
    "import math\n",
    "\n",
    "print(sorted((math.trunc(random.gauss(30, 10)) for x in range(100))))\n",
    "ages = []\n",
    "while len(ages) < n:\n",
    "    age = math.trunc(random.gauss(30, 10))\n",
    "    if age < 18:\n",
    "        continue\n",
    "        \n",
    "    if random.random() < 0.1:\n",
    "        age = None\n",
    "        \n",
    "    ages.append(age)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
