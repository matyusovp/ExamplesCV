#!/usr/bin/env python
# coding: utf-8

# In[ ]:


#Это наглядный пример моего подхода к созданию baseline модели с комментариями на русском и английском. 
#This is a clear example of my approach to creating a baseline model with comments in Russian and English.


# In[13]:


import os
import gc
import time
import math
import datetime
from math import log, floor
from sklearn.neighbors import KDTree

import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.utils import shuffle
import tqdm as tqdm

import seaborn as sns
from matplotlib import colors
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize
import plotly
import pickle 
import pywt
from statsmodels.robust import mad
import lightgbm as lgb
import scipy
import statsmodels
from scipy import signal
import statsmodels.api as sm
#from fbprophet import Prophet
from scipy.signal import butter, deconvolve
from statsmodels.tsa.arima_model import ARIMA
from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt
import os, sys, gc, warnings, psutil, random
import warnings
warnings.filterwarnings("ignore")


# In[2]:


#Изначальная загрузка данных больше не требуется, так как все сдамплено в пикл
#Starting data pack. Not used now, cause of pcl files.

#calendar = pd.read_csv('calendar.csv')
#selling_prices = pd.read_csv('sell_prices.csv')
#sample_submission = pd.read_csv('sample_submission.csv')
#sales_train_val = pd.read_csv('sales_train_validation.csv')


# In[ ]:


#Посмотрим на продажи некоторых случайных товаров, чтобы понять структуру данных
#Let's look at the sales of some random products to understand the data structure


# In[5]:


#В первую очередь получаем сортированный список ID , выбираем некоторые рандомные ID и строим по ним график продаж. 
#First of all , we get a sorted list of IDS, select some random IDS and male a graph.

ids = sorted(list(set(sales_train_val['id'])))
d_cols = [c for c in sales_train_val.columns if 'd_' in c]
x_1 = sales_train_val.loc[sales_train_val['id'] == ids[2]].set_index('id')[d_cols].values[0]
x_2 = sales_train_val.loc[sales_train_val['id'] == ids[1]].set_index('id')[d_cols].values[0]
x_3 = sales_train_val.loc[sales_train_val['id'] == ids[17]].set_index('id')[d_cols].values[0]

fig = make_subplots(rows=3, cols=1)

fig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,
                    mode='lines', name="First sample",
                         marker=dict(color="mediumseagreen")),
             row=1, col=1)

fig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,
                    mode='lines', name="Second sample",
                         marker=dict(color="violet")),
             row=2, col=1)

fig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,
                    mode='lines', name="Third sample",
                         marker=dict(color="dodgerblue")),
             row=3, col=1)

fig.update_layout(height=1200, width=800, title_text="Sample sales")
fig.show()


# In[ ]:


#Укрупним некоторые участика графика, для лучшего понимания
#Let's expand some parts of the graph for a better understanding


# In[6]:


ids = sorted(list(set(sales_train_val['id'])))
d_cols = [c for c in sales_train_val.columns if 'd_' in c]
x_1 = sales_train_val.loc[sales_train_val['id'] == ids[0]].set_index('id')[d_cols].values[0][:90]
x_2 = sales_train_val.loc[sales_train_val['id'] == ids[4]].set_index('id')[d_cols].values[0][1300:1400]
x_3 = sales_train_val.loc[sales_train_val['id'] == ids[65]].set_index('id')[d_cols].values[0][350:450]
fig = make_subplots(rows=3, cols=1)

fig.add_trace(go.Scatter(x=np.arange(len(x_1)), y=x_1, showlegend=False,
                    mode='lines+markers', name="First sample",
                         marker=dict(color="mediumseagreen")),
             row=1, col=1)

fig.add_trace(go.Scatter(x=np.arange(len(x_2)), y=x_2, showlegend=False,
                    mode='lines+markers', name="Second sample",
                         marker=dict(color="violet")),
             row=2, col=1)

fig.add_trace(go.Scatter(x=np.arange(len(x_3)), y=x_3, showlegend=False,
                    mode='lines+markers', name="Third sample",
                         marker=dict(color="dodgerblue")),
             row=3, col=1)

fig.update_layout(height=1200, width=800, title_text="Sample sales snippets")
fig.show()


# In[ ]:


#Уже сейчас видно, что продажах очень много нулевых значений. Возможно придется использовать свою функцию потерь.
#так же стоит разобраться с OutOfStock . Где реальные нулевые продажи, а где товара не было на складе. 

#You can already see that there are a lot of zero values in sales. You may have to use your own loss function.
#it's also worth dealing with OutOfStock . Where there are real zero sales, and where the product was not in stock.


# In[7]:


#Функция для удаления шума при работе с сигналами, подойдет нам, чтобы лучше понимать тренды продаж.
#The function for removing noise when working with signals is suitable for us to better understand sales trends.
#Not my code
def maddest(d, axis=None):
    return np.mean(np.absolute(d - np.mean(d, axis)), axis)

def denoise_signal(x, wavelet='db4', level=1):
    coeff = pywt.wavedec(x, wavelet, mode="per")
    sigma = (1/0.6745) * maddest(coeff[-level])

    uthresh = sigma * np.sqrt(2*np.log(len(x)))
    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])

    return pywt.waverec(coeff, wavelet, mode='per')


# Немного сгладим кривую продаж, для лучшего визуального восприятия 

# In[12]:


y_w1 = denoise_signal(x_1)
y_w2 = denoise_signal(x_2)
y_w3 = denoise_signal(x_3)


fig = make_subplots(rows=3, cols=1)

fig.add_trace(
    go.Scatter(x=np.arange(len(x_1)), mode='lines+markers', y=x_1, marker=dict(color="mediumaquamarine"), showlegend=False,
               name="Original signal"),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=np.arange(len(x_1)), y=y_w1, mode='lines', marker=dict(color="darkgreen"), showlegend=False,
               name="Denoised signal"),
    row=1, col=1
)

fig.add_trace(
    go.Scatter(x=np.arange(len(x_2)), mode='lines+markers', y=x_2, marker=dict(color="thistle"), showlegend=False),
    row=2, col=1
)

fig.add_trace(
    go.Scatter(x=np.arange(len(x_2)), y=y_w2, mode='lines', marker=dict(color="purple"), showlegend=False),
    row=2, col=1
)

fig.add_trace(
    go.Scatter(x=np.arange(len(x_3)), mode='lines+markers', y=x_3, marker=dict(color="lightskyblue"), showlegend=False),
    row=3, col=1
)

fig.add_trace(
    go.Scatter(x=np.arange(len(x_3)), y=y_w3, mode='lines', marker=dict(color="navy"), showlegend=False),
    row=3, col=1
)

fig.update_layout(height=1200, width=800, title_text="Original (pale) vs. Denoised (dark) sales")
fig.show()


# In[11]:


# Simple "Memory profilers" to see memory usage #Not my code
# Функция для понимания, сколько памяти занимает датафрейм
def get_memory_usage():
    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2) 
        
def sizeof_fmt(num, suffix='B'):
    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:
        if abs(num) < 1024.0:
            return "%3.1f%s%s" % (num, unit, suffix)
        num /= 1024.0
    return "%.1f%s%s" % (num, 'Yi', suffix)


# In[10]:


#Not my code
#Функция , которая работает с форматами колонок датафрейма и изменяет их на те, которые занимают меньше памяти. 
def reduce_mem_usage(df, verbose=True):
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage().sum() / 1024**2    
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                       df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)  
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)    
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))
    return df


# In[9]:


#Контакнтинация двух датафреймов
# Merging by concat 
def merge_by_concat(df1, df2, merge_on):
    merged_gf = df1[merge_on]
    merged_gf = merged_gf.merge(df2, on=merge_on, how='left')
    new_columns = [col for col in list(merged_gf) if col not in merge_on]
    df1 = pd.concat([df1, merged_gf[new_columns]], axis=1)
    return df1


# In[5]:


#####Первые глобальные переменные
#First Global VARS
TARGET = 'sales'         # Our main target Таргет
END_TRAIN = 1913         # Last day in train set Последний день в трейне.
MAIN_INDEX = ['id','d']  # We can identify item by these columns. Уникальный ключ. 


# In[7]:


#Make Grid
print('Create Grid')

# We can tranform horizontal representation 
# to vertical "view"
# Our "index" will be 'id','item_id','dept_id','cat_id','store_id','state_id'
# and labels are 'd_' coulmns

# Наши продажи это колонки d_, но они идут в горизонтальным порядке. Нам нужно трасформировать их, чтобы была одна колонка d
#с продажами. Для этого есть функция melt, мы указываем все индексы, которые трогать не надо и колонку, которую нужно развернуть
 
index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']
grid_df = pd.melt(sales_train_val, 
                  id_vars = index_columns, 
                  var_name = 'd', 
                  value_name = TARGET)

print('Train rows:', len(sales_train_val), len(grid_df))

# To be able to make predictions
# we need to add "test set" to our grid
#Добавим в хвост нашего трейна 28 пустых колонок (дней), продажи которых нам надо будет предсказывать.
add_grid = pd.DataFrame()
for i in range(1,29):
    temp_df = sales_train_val[index_columns]
    temp_df = temp_df.drop_duplicates()
    temp_df['d'] = 'd_'+ str(END_TRAIN+i)
    temp_df[TARGET] = np.nan
    add_grid = pd.concat([add_grid,temp_df])

grid_df = pd.concat([grid_df,add_grid])
grid_df = grid_df.reset_index(drop=True)

# Remove some temoprary DFs
#Удаляем ненужные структуры
del temp_df, add_grid
del sales_train_val


# Посмотрим сколько памяти занимает итоговая структура
print("{:>20}: {:>8}".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))

#Немного сэкономим памяти, переведя все в категорийный сет
for col in index_columns:
    grid_df[col] = grid_df[col].astype('category')

print("{:>20}: {:>8}".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))


# In[ ]:



print('Release week')
# Кажется, что некоторые нули в каджой строке трейна, это не реальные нулевые продажи, а отсутствие товара в магазине или OOS.
#есть множество различных методов работы с OOS. Удалять, заполнять средним, маркировать. Сейчас мы сделаем базовый предпроцессинг,
#мы уберем нули, когда товар еще не был в магазине с самого начала, так как еще не появлися в продаже. 

# It seems that leadings zero values  
# in each train_df item row
# are not real 0 sales but mean
# absence for the item in the store
# we can safe some memory by removing
# such zeros

# Prices are set by week
# Загрузим цены на товары и сгруппируем их по неделе, где товар первый раз появился в магазине. 

release_df = selling_prices.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()
release_df.columns = ['store_id','item_id','release']

# Now we can merge release_df
# Смержим основной датасет и датасет появления товара
grid_df = merge_by_concat(grid_df, release_df, ['store_id','item_id'])
del release_df

# У нас нет поля wm_yr_wk в основном датафрейме, поэтому смерщим его с календарем и достанем это поле.
grid_df = merge_by_concat(grid_df, calendar[['wm_yr_wk','d']], ['d'])
                      
# Теперь удаляем все нули, где дата меньше, чем дата первого появления товара, согласна структуре цен.  
# safe memory 
grid_df = grid_df[grid_df['wm_yr_wk']>=grid_df['release']]
grid_df = grid_df.reset_index(drop=True)


print("{:>20}: {:>8}".format('Original grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))

grid_df['release'] = grid_df['release'] - grid_df['release'].min()
grid_df['release'] = grid_df['release'].astype(np.int16)

print("{:>20}: {:>8}".format('Reduced grid_df',sizeof_fmt(grid_df.memory_usage(index=True).sum())))


# In[50]:


print('Save Part 1')
# Сделали базовый предпроцессинг
# и теперь можем сдампить в пикл

# We have our BASE grid ready
# and can save it as pickle file
# for future use (model training)

grid_df.to_pickle('grid_part_1.pkl')

print('Size:', grid_df.shape)


# In[ ]:


#Создаем дополнительные базовые фичи в структуре цен
#Create some new features
print('Prices')

# We can do some basic aggregations
# Базовые агрегации

selling_prices['price_max'] = selling_prices.groupby(['store_id','item_id'])['sell_price'].transform('max')
selling_prices['price_min'] = selling_prices.groupby(['store_id','item_id'])['sell_price'].transform('min')
selling_prices['price_std'] = selling_prices.groupby(['store_id','item_id'])['sell_price'].transform('std')
selling_prices['price_mean'] = selling_prices.groupby(['store_id','item_id'])['sell_price'].transform('mean')

# (min/max scaling) Нормализация
selling_prices['price_norm'] = selling_prices['sell_price']/selling_prices['price_max']

# Some items are can be inflation dependent
# and some items are very "stable"
# Некоторые товары меняют цены очень часто, а некоторые устойчивые.

selling_prices['price_nunique'] = selling_prices.groupby(['store_id','item_id'])['sell_price'].transform('nunique')
selling_prices['item_nunique'] = selling_prices.groupby(['store_id','sell_price'])['item_id'].transform('nunique')

# Сджойним из календаря так же месяц и год. 
calendar_prices = calendar[['wm_yr_wk','month','year']]
calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])
selling_prices = selling_prices.merge(calendar_prices[['wm_yr_wk','month','year']], on=['wm_yr_wk'], how='left')
del calendar_prices

# Дополнительные фичи в разрезе месяца и года. 
selling_prices['price_momentum'] = selling_prices['sell_price']/selling_prices.groupby(['store_id','item_id'])['sell_price'].transform(lambda x: x.shift(1))
selling_prices['price_momentum_m'] = selling_prices['sell_price']/selling_prices.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')
selling_prices['price_momentum_y'] = selling_prices['sell_price']/selling_prices.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')

del selling_prices['month'], selling_prices['year']


# In[ ]:


# Мерж фичей с основной структурой

print('Merge prices and save part 2')

# Merge Prices
original_columns = list(grid_df)
grid_df = grid_df.merge(selling_prices, on=['store_id','item_id','wm_yr_wk'], how='left')
keep_columns = [col for col in list(grid_df) if col not in original_columns]
grid_df = grid_df[MAIN_INDEX+keep_columns]
grid_df = reduce_mem_usage(grid_df)

# Safe part 2
grid_df.to_pickle('grid_part_2.pkl')
print('Size:', grid_df.shape)


del selling_prices

# Загрузим первоначальную структуру, для создание новых фичей.
grid_df = pd.read_pickle('grid_part_1.pkl')


# In[7]:


from math import ceil


# In[ ]:


#Merge calendar
#Добавляем фичи из структуры календаря. 
grid_df = grid_df[MAIN_INDEX]


icols = ['date',
         'd',
         'event_name_1',
         'event_type_1',
         'event_name_2',
         'event_type_2',
         'snap_CA',
         'snap_TX',
         'snap_WI']

grid_df = grid_df.merge(calendar[icols], on=['d'], how='left')

# Конверитурем ячейки в категорийные
#  convert to bool or int8
icols = ['event_name_1',
         'event_type_1',
         'event_name_2',
         'event_type_2',
         'snap_CA',
         'snap_TX',
         'snap_WI']
for col in icols:
    grid_df[col] = grid_df[col].astype('category')

# Convert to DateTime
grid_df['date'] = pd.to_datetime(grid_df['date'])

# Make some features from date
# Создаем необходимые нам фичи из календаря. 
grid_df['tm_d'] = grid_df['date'].dt.day.astype(np.int8)
grid_df['tm_w'] = grid_df['date'].dt.week.astype(np.int8)
grid_df['tm_m'] = grid_df['date'].dt.month.astype(np.int8)
grid_df['tm_y'] = grid_df['date'].dt.year
grid_df['tm_y'] = (grid_df['tm_y'] - grid_df['tm_y'].min()).astype(np.int8)
grid_df['tm_wm'] = grid_df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)

grid_df['tm_dw'] = grid_df['date'].dt.dayofweek.astype(np.int8)
grid_df['tm_w_end'] = (grid_df['tm_dw']>=5).astype(np.int8)

# Remove date
del grid_df['date']
#Эти фичи сдамплены в grid_part_3.pkl


# In[76]:


#not my idea and code

grid_df = pd.read_pickle('grid_part_1.pkl')
grid_df['d'] = grid_df['d'].apply(lambda x: x[2:]).astype(np.int16)

del grid_df['wm_yr_wk']
grid_df.to_pickle('grid_part_1.pkl')

del grid_df


# In[12]:


# Загрузим все три части нашего датафрема и посмотрим на них 
grid_df = pd.concat([pd.read_pickle('grid_part_1.pkl'),
                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],
                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],
                     axis=1)
                     
# Let's check again memory usage
print("{:>20}: {:>8}".format('Full Grid',sizeof_fmt(grid_df.memory_usage(index=True).sum())))
print('Size:', grid_df.shape)
grid_df[:10]


# In[ ]:


#Not my code
#Некоторый пример мультипроцессинга создания переменных. 
from multiprocessing import Pool                # Multiprocess Runs
def df_parallelize_run(func, t_split):
    num_cores = np.min([N_CORES,len(t_split)])
    pool = Pool(num_cores)
    df = pd.concat(pool.map(func, t_split), axis=1)
    pool.close()
    pool.join()
    return df
def make_normal_lag(lag_day):
    lag_df = grid_df[['id','d',TARGET]] # not good to use df from "global space"
    col_name = 'sales_lag_'+str(lag_day)
    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)
    return lag_df[[col_name]]
LAGS_SPLIT = [col for col in range(1,1+7)]
grid_df = pd.concat([grid_df, df_parallelize_run(make_normal_lag,LAGS_SPLIT)], axis=1)
test_model = make_fast_test(grid_df)


# In[26]:


# Это ячейка тестовая, для проверки функции создании лагов. 

#Не получилось использовать мультироцессинг, надо разбираться. Но функциоей посоздавать лаги продаж
#можно как пример: используем отсортированный массив
def make_normal_lag(lag_day):
    lag_df = grid_df[['id','d',TARGET]] # не используем глобальную переменную
    col_name = 'sales_lag_'+str(lag_day)# Название колонки со сдвигом назад
    # Тут группируем наш датафрейм, по тому, что нам надо(id) и указываем переменную , значение которой нужно посмотреть
    #из предыдущей строки 
    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(lag_day)).astype(np.float16)
    return lag_df[[col_name]]
#Создаем 7 новых переменных, продажи назад
for i in range(1,1+7):
    grid_df = pd.concat([grid_df, make_normal_lag(i)], axis=1)


# In[43]:


#Созданием новых фичей, 28 дней отрезаем как будующий тест. 
#Creating new features, 28 days befire train end are our test
grid_df = pd.read_pickle('grid_part_1.pkl')
grid_df[TARGET][grid_df['d']>(1913-28)] = np.nan
base_cols = list(grid_df)

#Колонки по которым будем агрегировать продажи и считать в их агрегации различные метрики
#Cols for agg functions
icols =  [
            ['state_id'],
            ['store_id'],
            ['cat_id'],
            ['dept_id'],
            ['state_id', 'cat_id'],
            ['state_id', 'dept_id'],
            ['store_id', 'cat_id'],
            ['store_id', 'dept_id'],
            ['item_id'],
            ['item_id', 'state_id'],
            ['item_id', 'store_id']
            ]

for col in icols:
    print('Encoding', col)
    col_name = '_'+'_'.join(col)+'_'
    grid_df['enc'+col_name+'mean'] = grid_df.groupby(col)[TARGET].transform('mean').astype(np.float16)
    grid_df['enc'+col_name+'std'] = grid_df.groupby(col)[TARGET].transform('std').astype(np.float16)

keep_cols = [col for col in list(grid_df) if col not in base_cols]
grid_df = grid_df[['id','d']+keep_cols]


# In[44]:


#cохраняем
print('Save Mean/Std encoding')
grid_df.to_pickle('mean_encoding_df.pkl')


# In[45]:


grid_df.info()


# ROLLING LAGS. Создаем множество различный фичей, сколько товаров было продано вчера, 7 дней назад,  сколько продавалось на той неделе год назад и так далее. 

# In[46]:



grid_df = pd.read_pickle('grid_part_1.pkl')
# We need only 'id','d','sales'
# Памяти мало, поэтом сохраним только айдишники и преобразуемый таргет.
grid_df = grid_df[['id','d','sales']]
SHIFT_DAY = 28 # так как предсказываем на 28 дней вперед, то сдвиг у нас будет минимальный 28 дней назад. 

# Lags
# with 28 day shift
start_time = time.time()
print('Create lags')

LAG_DAYS = [col for col in range(SHIFT_DAY,SHIFT_DAY+15)]
grid_df = grid_df.assign(**{
        '{}_lag_{}'.format(col, l): grid_df.groupby(['id'])[col].transform(lambda x: x.shift(l))
        for l in LAG_DAYS
        for col in [TARGET]
    })

# Minify
for col in list(grid_df):
    if 'lag' in col:
        grid_df[col] = grid_df[col].astype(np.float16)

print('%0.2f min: Lags' % ((time.time() - start_time) / 60))

# Rollings
# with 28 day shift
start_time = time.time()
print('Create rolling aggs')

for i in [7,14,30,60,180]:
    print('Rolling period:', i)
    grid_df['rolling_mean_'+str(i)] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).mean()).astype(np.float16)
    grid_df['rolling_std_'+str(i)]  = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(SHIFT_DAY).rolling(i).std()).astype(np.float16)

# Rollings
# with sliding shift
for d_shift in [1,7,14]: 
    print('Shifting period:', d_shift)
    for d_window in [7,14,30,60]:
        col_name = 'rolling_mean_tmp_'+str(d_shift)+'_'+str(d_window)
        grid_df[col_name] = grid_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(d_shift).rolling(d_window).mean()).astype(np.float16)
    
    
print('%0.2f min: Lags' % ((time.time() - start_time) / 60))


# In[47]:


print('Save lags and rollings')
grid_df.to_pickle('lags_df_'+str(SHIFT_DAY)+'.pkl')


# Prediction PART

# In[25]:


#Предсказывать и обучать будет для каждого магазина отдельно. 
#Helper to load data by store ID

def get_data_by_store(store):
    
    # Read and contact basic feature
    # Загружаем все фичи, сгенерированные ранее и конкантинируем
    df = pd.concat([pd.read_pickle('grid_part_1.pkl'),
                     pd.read_pickle('grid_part_2.pkl').iloc[:,2:],
                     pd.read_pickle('grid_part_3.pkl').iloc[:,2:]],
                     axis=1)
    
    
    df = df[df['store_id']==store]
    #Загружаем фичи 1 
    df2 = pd.read_pickle('mean_encoding_df.pkl')[mean_features]
    #mean_features будут указаны позже вне функции
    df2 = df2[df2.index.isin(df.index)]
    #Загружаем фичи 1 
    df3 = pd.read_pickle('lags_df_28.pkl').iloc[:,3:]
    df3 = df3[df3.index.isin(df.index)]
    #Конкантинируем все в 1 датасет с данными по магазину
    df = pd.concat([df, df2], axis=1)
    del df2 
    
    df = pd.concat([df, df3], axis=1)
    del df3 #Конкантинируем все в 1 датасет с данными по магазину
    
    # Create features list \ Создаем список переменных для обучения
    features = [col for col in list(df) if col not in remove_features]
    #remove_features будут указаны позже вне функции
    df = df[['id','d',TARGET]+features]
    
    # Skipping first n rows, создаем возможность обучаться не с нулевого дня. 
    df = df[df['d']>=START_TRAIN].reset_index(drop=True)
    
    return df, features

# Recombine Test set after training
# так как обучаться будем на каждом магазине, то отдельно для каждого магазина нужен будет тестовый набор данных с фичами
def get_base_test():
    base_test = pd.DataFrame()

    for store_id in STORES_IDS:
        temp_df = pd.read_pickle('test_'+store_id+'.pkl')
        temp_df['store_id'] = store_id
        base_test = pd.concat([base_test, temp_df]).reset_index(drop=True)
    
    return base_test


# Helper to make dynamic rolling lags
# еще несколько функция для создания фичей в тестовом наборе данных. 

def make_lag(LAG_DAY):
    lag_df = base_test[['id','d',TARGET]]
    col_name = 'sales_lag_'+str(LAG_DAY)
    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(LAG_DAY)).astype(np.float16)
    return lag_df[[col_name]]


def make_lag_roll(LAG_DAY):
    shift_day = LAG_DAY[0]
    roll_wind = LAG_DAY[1]
    lag_df = base_test[['id','d',TARGET]]
    col_name = 'rolling_mean_tmp_'+str(shift_day)+'_'+str(roll_wind)
    lag_df[col_name] = lag_df.groupby(['id'])[TARGET].transform(lambda x: x.shift(shift_day).rolling(roll_wind).mean())
    return lag_df[[col_name]]


# In[2]:


df3 = pd.read_pickle('lags_df_28.pkl').iloc[:,3:]


# In[4]:


df3.tail()


# In[21]:


lgb_params = {
                    'boosting_type': 'gbdt',
                    'objective': 'tweedie',
                    'tweedie_variance_power': 1.1,
                    'metric': 'rmse',
                    'subsample': 0.5,
                    'subsample_freq': 1,
                    'learning_rate': 0.03,
                    'num_leaves': 2**11-1,
                    'min_data_in_leaf': 2**12-1,
                    'feature_fraction': 0.5,
                    'max_bin': 100,
                    'n_estimators': 1400,
                    'boost_from_average': False,
                    'verbose': -1,
                } 

#Разберем параметры LGBM

## 'boosting_type': 'gbdt'
# У нас есть вариант goss, но он часто недообучен. Вариант dart - очень долгий и рандомный
# Подробнее можно почитать тут
# https://www.kaggle.com/c/home-credit-default-risk/discussion/60921

## 'objective': 'tweedie'
# Tweedie Gradient Boosting for Extremely
# Unbalanced Zero-inflated Data
# https://arxiv.org/pdf/1811.10192.pdf
# tweedie отлично показал себя на CV, в дальнейшем следует написать свою функцию My advice here - make OWN LOSS function
# Try to figure out why Tweedie works.


## 'tweedie_variance_power': 1.1
# default = 1.5
# set this closer to 2 to shift towards a Gamma distribution
# set this closer to 1 to shift towards a Poisson distribution


## 'metric': 'rmse' Стандарт


## 'subsample': 0.5
# Стараемся не оферфитнуть модель, это рандомное число, где мой CV показал лучший результат
# this will randomly select part of data without resampling


##'subsample_freq': 1


## 'learning_rate': 0.03




## 'num_leaves': 2**11-1
## 'min_data_in_leaf': 2**12-1
# Заставляем модель использовать как можно больше переменных.
# Это может вызвать переобучение, но для этого выставляем максимальное кол-во корзин 100

# 'max_bin': 100

                    
## 'n_estimators': 1400


##'feature_fraction': 0.5
# У нас очень много переменных и многие из них мусорные, так что заставляем брать очень разные части фичей.
# We have maaaany features
# and many of them are "duplicates"
# and many just "noise"
# good values here - 0.5-0.7 (by CV)


# Вот что пишут в интернетах по поводу усреднения
## 'boost_from_average': False
# There is some "problem"
# to code boost_from_average for 
# custom loss
# 'True' makes training faster
# BUT carefull use it
# https://github.com/microsoft/LightGBM/issues/1514
# not our case but good to know cons


# In[24]:


# Helpers/ Статичность и паралельный запуск
def seed_everything(seed=0):
    random.seed(seed)
    np.random.seed(seed)

    
## Multiprocess Runs
def df_parallelize_run(func, t_split):
    num_cores = np.min([N_CORES,len(t_split)])
    pool = Pool(num_cores)
    df = pd.concat(pool.map(func, t_split), axis=1)
    pool.close()
    pool.join()
    return df


# In[22]:


#Устанавливаем глобальные переменные. Var2 /. 
VER = 1                          # Версия модели
SEED = 42                        # Задаем сиды.
seed_everything(SEED)            
lgb_params['seed'] = SEED        
N_CORES = psutil.cpu_count()     # Доступные цпу



TARGET      = 'sales'            # Наш таргет
START_TRAIN = 0                  # Начало обучение ( день)
END_TRAIN   = 1913               # Когда обучение заканчивается ( последний день трейна)
P_HORIZON   = 28                 # Prediction horizon ( на сколько дней вперед предсказывать)
USE_AUX     = False               # Use or not pretrained models ( пока не нужно.)

#FEATURES to remove.

remove_features = ['id','state_id','store_id',
                   'date','wm_yr_wk','d',TARGET]
mean_features   = ['enc_cat_id_mean','enc_cat_id_std',
                   'enc_dept_id_mean','enc_dept_id_std',
                   'enc_item_id_mean','enc_item_id_std'] 

#PATHS for Features, тут хранятся все наши созданные датафреймы и фичи
ORIGINAL = 'sales_train_evaluation.csv'
BASE     = 'grid_part_1.pkl'
PRICE    = 'grid_part_2.pkl'
CALENDAR = 'grid_part_3.pkl'
LAGS     = 'lags_df_28.pkl'
MEAN_ENC = 'mean_encoding_df.pkl'


# AUX(pretrained) Models paths
#AUX_MODELS = '.'


#STORES ids
STORES_IDS = pd.read_csv(ORIGINAL)['store_id']
STORES_IDS = list(STORES_IDS.unique())


#SPLITS for lags creation
SHIFT_DAY  = 28
N_LAGS     = 15
LAGS_SPLIT = [col for col in range(SHIFT_DAY,SHIFT_DAY+N_LAGS)]
ROLS_SPLIT = []
for i in [1,7,14]:
    for j in [7,14,30,60]:
        ROLS_SPLIT.append([i,j])


# In[22]:


#Train Models

for store_id in STORES_IDS:
    print('Train', store_id)
    
    # Get grid for current store
    # Для каждого магазины получаем его отдельный датафрейм и список всех фичей для обучения
    grid_df, features_columns = get_data_by_store(store_id)
    #Создаем маски для наших переменных. 
    
    # Train (All data less than 1913) Обучаемся на всем до 1913 дня.
    # "Validation" (Last 28 days - not real validatio set) Валидируемся на последних 28 дня. 1884 - 1912
    # Test (All data greater than 1913 day, 
    #       with some gap for recursive features) Тестовые данные это все данные после 1913 дня, но с небольим запасом,
    # чтобы можно было посчитать скользящие среднии и т.д. 
    train_mask = grid_df['d']<=END_TRAIN
    valid_mask = train_mask&(grid_df['d']>(END_TRAIN-P_HORIZON))
    preds_mask = grid_df['d']>(END_TRAIN-100)
    
    # Применяем маски.Сохраняя трейн в бин.

    train_data = lgb.Dataset(grid_df[train_mask][features_columns], 
                       label=grid_df[train_mask][TARGET])
    train_data.save_binary('train_data.bin')
    train_data = lgb.Dataset('train_data.bin')
    
    # Применяем маски.Создаем валидационный набор
    
    valid_data = lgb.Dataset(grid_df[valid_mask][features_columns], 
                       label=grid_df[valid_mask][TARGET])
    
    # Применяем маски.Создаем тестовый набор данных. Удаляем фичи, которые нам нужно посчитать рекурсивно
    # 
    grid_df = grid_df[preds_mask].reset_index(drop=True)
    keep_cols = [col for col in list(grid_df) if '_tmp_' not in col]
    grid_df = grid_df[keep_cols]
    grid_df.to_pickle('test_'+store_id+'.pkl')
    del grid_df
    

    seed_everything(SEED)
    estimator = lgb.train(lgb_params,
                          train_data,
                          valid_sets = [valid_data],
                          verbose_eval = 100,
                          )
    

    model_name = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin'
    pickle.dump(estimator, open(model_name, 'wb'))

    # Remove temporary files and objects 
    # to free some hdd space and ram memory
    get_ipython().system('rm train_data.bin')
    del train_data, valid_data, estimator
    gc.collect()
    
    # "Keep" models features for predictions
    MODEL_FEATURES = features_columns


# In[ ]:


########################### Predict
#################################################################################
STORES_IDS = pd.read_csv(ORIGINAL)['store_id']
STORES_IDS = list(STORES_IDS.unique())
# Create Dummy DataFrame to store predictions
all_preds = pd.DataFrame()

# Join back the Test dataset with 
# a small part of the training data 
# to make recursive features
base_test = get_base_test()

# Timer to measure predictions time 
main_time = time.time()

# Loop over each prediction day
# As rolling lags are the most timeconsuming
# we will calculate it for whole day
for PREDICT_DAY in range(1,29):    
    print('Predict | Day:', PREDICT_DAY)
    start_time = time.time()

    # Make temporary grid to calculate rolling lags
    grid_df = base_test.copy()
    grid_df = pd.concat([grid_df, df_parallelize_run(make_lag_roll, ROLS_SPLIT)], axis=1)
        
    for store_id in STORES_IDS:
        
        # Read all our models and make predictions
        # for each day/store pairs
        model_path = 'lgb_model_'+store_id+'_v'+str(VER)+'.bin' 
        if USE_AUX:
            model_path = AUX_MODELS + model_path
        
        estimator = pickle.load(open(model_path, 'rb'))
        
        day_mask = base_test['d']==(END_TRAIN+PREDICT_DAY)
        store_mask = base_test['store_id']==store_id
        
        mask = (day_mask)&(store_mask)
        base_test[TARGET][mask] = estimator.predict(grid_df[mask][MODEL_FEATURES])
    
    # Make good column naming and add 
    # to all_preds DataFrame
    temp_df = base_test[day_mask][['id',TARGET]]
    temp_df.columns = ['id','F'+str(PREDICT_DAY)]
    if 'id' in list(all_preds):
        all_preds = all_preds.merge(temp_df, on=['id'], how='left')
    else:
        all_preds = temp_df.copy()
        
    print('#'*10, ' %0.2f min round |' % ((time.time() - start_time) / 60),
                  ' %0.2f min total |' % ((time.time() - main_time) / 60),
                  ' %0.2f day sales |' % (temp_df['F'+str(PREDICT_DAY)].sum()))
    del temp_df
    
all_preds = all_preds.reset_index(drop=True)
all_preds

